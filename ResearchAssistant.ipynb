{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTTUycacUm5wTWvJhDc8U/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mudit-Sharma24/IIT_Project/blob/main/ResearchAssistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai requests pathlib dataclasses argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "jTZmTUs0RfMT",
        "outputId": "29db5cdb-a2e3-45dd-ceee-153e0888642b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.12/dist-packages (0.6)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "6e23f6b0b2694e549f14611d04da7eac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal Research Assistant - Google Colab Version with Gemini API\n",
        "# Fetches academic papers, summarizes key findings, and organizes notes using Google's Gemini API\n",
        "\n",
        "# ========================================\n",
        "# INSTALLATION AND SETUP\n",
        "# ========================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install google-generativeai requests xmltodict python-dotenv -q\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import xml.etree.ElementTree as ET\n",
        "from urllib.parse import quote\n",
        "import google.generativeai as genai\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Configure logging for Colab\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ========================================\n",
        "# DATA CLASSES AND CONFIGURATIONS\n",
        "# ========================================\n",
        "\n",
        "@dataclass\n",
        "class Paper:\n",
        "    \"\"\"Data class for academic papers\"\"\"\n",
        "    title: str\n",
        "    authors: List[str]\n",
        "    abstract: str\n",
        "    url: str\n",
        "    published_date: str\n",
        "    source: str\n",
        "    doi: Optional[str] = None\n",
        "    categories: List[str] = None\n",
        "\n",
        "class ColabConfig:\n",
        "    \"\"\"Configuration class for Colab environment\"\"\"\n",
        "    def __init__(self):\n",
        "        self.base_path = Path(\"/content/research_projects\")\n",
        "        self.base_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def setup_gemini_key(self):\n",
        "        \"\"\"Interactive setup for Gemini API key\"\"\"\n",
        "        from getpass import getpass\n",
        "\n",
        "        print(\"Google Gemini API Key Setup\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"Get your FREE API key at: https://aistudio.google.com/app/apikey\")\n",
        "        print()\n",
        "\n",
        "        # Check if key already exists in environment\n",
        "        if os.getenv('GOOGLE_API_KEY'):\n",
        "            print(\"Gemini API key already configured!\")\n",
        "            return os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "        # Interactive key input\n",
        "        api_key = getpass(\"Enter your Google Gemini API key (hidden input): \")\n",
        "\n",
        "        if not api_key or len(api_key) < 20:\n",
        "            raise ValueError(\"Invalid Gemini API key format. Please check your key.\")\n",
        "\n",
        "        # Set environment variable\n",
        "        os.environ['GOOGLE_API_KEY'] = api_key\n",
        "        genai.configure(api_key=api_key)\n",
        "        print(\"API key configured successfully!\")\n",
        "        return api_key\n",
        "\n",
        "# ========================================\n",
        "# PAPER FETCHING CLASSES\n",
        "# ========================================\n",
        "\n",
        "class PaperFetcher:\n",
        "    \"\"\"Handles fetching papers from various academic sources\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Research Assistant Bot 1.0 (Educational Purpose)'\n",
        "        })\n",
        "\n",
        "    def fetch_arxiv_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from ArXiv\"\"\"\n",
        "        print(f\"Searching ArXiv for: '{query}'\")\n",
        "\n",
        "        base_url = \"http://export.arxiv.org/api/query\"\n",
        "        params = {\n",
        "            'search_query': f\"all:{query}\",\n",
        "            'start': 0,\n",
        "            'max_results': max_results,\n",
        "            'sortBy': 'submittedDate',\n",
        "            'sortOrder': 'descending'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(base_url, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            papers = []\n",
        "            root = ET.fromstring(response.content)\n",
        "\n",
        "            for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "                title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
        "                abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()\n",
        "\n",
        "                authors = []\n",
        "                for author in entry.findall('{http://www.w3.org/2005/Atom}author'):\n",
        "                    name = author.find('{http://www.w3.org/2005/Atom}name').text\n",
        "                    authors.append(name)\n",
        "\n",
        "                published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
        "                url = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
        "\n",
        "                # Extract categories\n",
        "                categories = []\n",
        "                for category in entry.findall('{http://www.w3.org/2005/Atom}category'):\n",
        "                    categories.append(category.get('term'))\n",
        "\n",
        "                paper = Paper(\n",
        "                    title=title,\n",
        "                    authors=authors,\n",
        "                    abstract=abstract,\n",
        "                    url=url,\n",
        "                    published_date=published,\n",
        "                    source=\"ArXiv\",\n",
        "                    categories=categories\n",
        "                )\n",
        "                papers.append(paper)\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from ArXiv\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching ArXiv papers: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_semantic_scholar_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from Semantic Scholar\"\"\"\n",
        "        print(f\"Searching Semantic Scholar for: '{query}'\")\n",
        "\n",
        "        base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'limit': max_results,\n",
        "            'fields': 'title,authors,abstract,url,publicationDate,externalIds'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(base_url, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            papers = []\n",
        "            for item in data.get('data', []):\n",
        "                if not item.get('abstract'):\n",
        "                    continue\n",
        "\n",
        "                authors = [author.get('name', '') for author in item.get('authors', [])]\n",
        "                external_ids = item.get('externalIds', {})\n",
        "                doi = external_ids.get('DOI')\n",
        "\n",
        "                paper = Paper(\n",
        "                    title=item.get('title', ''),\n",
        "                    authors=authors,\n",
        "                    abstract=item.get('abstract', ''),\n",
        "                    url=item.get('url', ''),\n",
        "                    published_date=item.get('publicationDate', ''),\n",
        "                    source=\"Semantic Scholar\",\n",
        "                    doi=doi\n",
        "                )\n",
        "                papers.append(paper)\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from Semantic Scholar\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching Semantic Scholar papers: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_pubmed_papers(self, query: str, max_results: int = 10) -> List[Paper]:\n",
        "        \"\"\"Fetch papers from PubMed (bonus source)\"\"\"\n",
        "        print(f\"Searching PubMed for: '{query}'\")\n",
        "\n",
        "        # First, search for paper IDs\n",
        "        search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "        search_params = {\n",
        "            'db': 'pubmed',\n",
        "            'term': query,\n",
        "            'retmax': max_results,\n",
        "            'retmode': 'json',\n",
        "            'sort': 'pub_date'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            search_response = self.session.get(search_url, params=search_params, timeout=30)\n",
        "            search_response.raise_for_status()\n",
        "            search_data = search_response.json()\n",
        "\n",
        "            paper_ids = search_data.get('esearchresult', {}).get('idlist', [])\n",
        "\n",
        "            if not paper_ids:\n",
        "                print(\"No papers found from PubMed\")\n",
        "                return []\n",
        "\n",
        "            # Fetch paper details\n",
        "            fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "            fetch_params = {\n",
        "                'db': 'pubmed',\n",
        "                'id': ','.join(paper_ids),\n",
        "                'retmode': 'xml'\n",
        "            }\n",
        "\n",
        "            fetch_response = self.session.get(fetch_url, params=fetch_params, timeout=30)\n",
        "            fetch_response.raise_for_status()\n",
        "\n",
        "            papers = []\n",
        "            root = ET.fromstring(fetch_response.content)\n",
        "\n",
        "            for article in root.findall('.//PubmedArticle'):\n",
        "                try:\n",
        "                    # Extract title\n",
        "                    title_elem = article.find('.//ArticleTitle')\n",
        "                    title = title_elem.text if title_elem is not None else \"No title\"\n",
        "\n",
        "                    # Extract abstract\n",
        "                    abstract_elem = article.find('.//AbstractText')\n",
        "                    abstract = abstract_elem.text if abstract_elem is not None else \"No abstract available\"\n",
        "\n",
        "                    # Extract authors\n",
        "                    authors = []\n",
        "                    for author in article.findall('.//Author'):\n",
        "                        lastname = author.find('LastName')\n",
        "                        firstname = author.find('ForeName')\n",
        "                        if lastname is not None and firstname is not None:\n",
        "                            authors.append(f\"{firstname.text} {lastname.text}\")\n",
        "\n",
        "                    # Extract publication date\n",
        "                    pub_date = article.find('.//PubDate/Year')\n",
        "                    published_date = pub_date.text if pub_date is not None else \"Unknown\"\n",
        "\n",
        "                    # Extract PMID for URL\n",
        "                    pmid_elem = article.find('.//PMID')\n",
        "                    pmid = pmid_elem.text if pmid_elem is not None else \"\"\n",
        "                    url = f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\" if pmid else \"\"\n",
        "\n",
        "                    paper = Paper(\n",
        "                        title=title,\n",
        "                        authors=authors,\n",
        "                        abstract=abstract,\n",
        "                        url=url,\n",
        "                        published_date=published_date,\n",
        "                        source=\"PubMed\"\n",
        "                    )\n",
        "                    papers.append(paper)\n",
        "\n",
        "                except Exception as e:\n",
        "                    continue  # Skip problematic papers\n",
        "\n",
        "            print(f\"Found {len(papers)} papers from PubMed\")\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching PubMed papers: {e}\")\n",
        "            return []\n",
        "\n",
        "# ========================================\n",
        "# SUMMARIZATION CLASS WITH GEMINI\n",
        "# ========================================\n",
        "\n",
        "class GeminiResearchSummarizer:\n",
        "    \"\"\"Handles summarization using Google's Gemini API\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def summarize_paper(self, paper: Paper) -> Dict[str, str]:\n",
        "        \"\"\"Generate a comprehensive summary of a paper\"\"\"\n",
        "        print(f\"Summarizing: {paper.title[:60]}...\")\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Please analyze this academic paper and provide a structured summary in JSON format:\n",
        "\n",
        "        Title: {paper.title}\n",
        "        Authors: {', '.join(paper.authors)}\n",
        "        Abstract: {paper.abstract}\n",
        "        Source: {paper.source}\n",
        "\n",
        "        Provide your analysis in the following JSON format:\n",
        "        {{\n",
        "            \"key_findings\": \"Main discoveries and results (2-3 sentences)\",\n",
        "            \"methodology\": \"Research methods used (1-2 sentences)\",\n",
        "            \"significance\": \"Why this research matters (1-2 sentences)\",\n",
        "            \"limitations\": \"Study limitations or future work needed (1-2 sentences)\",\n",
        "            \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"keyword5\"],\n",
        "            \"summary\": \"Comprehensive overview (4-5 sentences)\",\n",
        "            \"practical_applications\": \"Real-world applications (1-2 sentences)\"\n",
        "        }}\n",
        "\n",
        "        Be concise but informative. Focus on the most important aspects. Return only valid JSON.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            content = response.text.strip()\n",
        "\n",
        "            # Try to parse JSON response\n",
        "            try:\n",
        "                # Clean the response to extract JSON\n",
        "                if '```json' in content:\n",
        "                    content = content.split('```json')[1].split('```')[0]\n",
        "                elif '```' in content:\n",
        "                    content = content.split('```')[1].split('```')[0]\n",
        "\n",
        "                return json.loads(content)\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback summary\n",
        "                return {\n",
        "                    \"key_findings\": \"Could not parse structured analysis\",\n",
        "                    \"methodology\": \"Not specified\",\n",
        "                    \"significance\": \"Not specified\",\n",
        "                    \"limitations\": \"Not specified\",\n",
        "                    \"keywords\": [\"analysis\", \"research\", \"academic\"],\n",
        "                    \"summary\": content[:500] + \"...\" if len(content) > 500 else content,\n",
        "                    \"practical_applications\": \"Not specified\"\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing paper: {e}\")\n",
        "            return {\n",
        "                \"key_findings\": \"Summary generation failed\",\n",
        "                \"methodology\": \"Not available\",\n",
        "                \"significance\": \"Not available\",\n",
        "                \"limitations\": \"Not available\",\n",
        "                \"keywords\": [\"error\"],\n",
        "                \"summary\": paper.abstract[:500] + \"...\" if len(paper.abstract) > 500 else paper.abstract,\n",
        "                \"practical_applications\": \"Not available\"\n",
        "            }\n",
        "\n",
        "    def generate_literature_review(self, papers: List[Paper], summaries: List[Dict], query: str) -> str:\n",
        "        \"\"\"Generate a comprehensive literature review\"\"\"\n",
        "        print(\"Generating literature review...\")\n",
        "\n",
        "        paper_summaries = []\n",
        "        for paper, summary in zip(papers, summaries):\n",
        "            paper_summaries.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors[:3],  # Limit authors for brevity\n",
        "                \"key_findings\": summary.get(\"key_findings\", \"\"),\n",
        "                \"significance\": summary.get(\"significance\", \"\"),\n",
        "                \"keywords\": summary.get(\"keywords\", [])[:5]\n",
        "            })\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Write a comprehensive literature review based on the following research papers about \"{query}\":\n",
        "\n",
        "        Number of papers analyzed: {len(papers)}\n",
        "\n",
        "        Paper summaries:\n",
        "        {json.dumps(paper_summaries, indent=2)}\n",
        "\n",
        "        Your literature review should include:\n",
        "\n",
        "        1. **Introduction** - Overview of the research area and its importance\n",
        "        2. **Current State of Research** - Key themes and findings across studies\n",
        "        3. **Methodological Approaches** - Common research methods used\n",
        "        4. **Key Findings and Trends** - Major discoveries and patterns\n",
        "        5. **Gaps and Limitations** - Areas needing further research\n",
        "        6. **Future Directions** - Suggestions for upcoming research\n",
        "        7. **Conclusion** - Summary of the field's current status\n",
        "\n",
        "        Write in an academic style, approximately 1000-1200 words. Use clear headings and make connections between different studies.\n",
        "        Cite papers by their titles when referencing specific findings.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating literature review: {e}\")\n",
        "            return f\"# Literature Review: {query}\\n\\nLiterature review generation encountered an error. Please review individual paper summaries for insights.\"\n",
        "\n",
        "    def generate_research_insights(self, papers: List[Paper], summaries: List[Dict], query: str) -> str:\n",
        "        \"\"\"Generate additional research insights and trends\"\"\"\n",
        "        print(\"Generating research insights...\")\n",
        "\n",
        "        keywords_all = []\n",
        "        for summary in summaries:\n",
        "            keywords_all.extend(summary.get(\"keywords\", []))\n",
        "\n",
        "        # Count keyword frequency\n",
        "        keyword_counts = {}\n",
        "        for keyword in keywords_all:\n",
        "            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1\n",
        "\n",
        "        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Based on the research about \"{query}\", provide insights about trends and patterns:\n",
        "\n",
        "        Total papers analyzed: {len(papers)}\n",
        "        Top keywords found: {[kw[0] for kw in top_keywords[:5]]}\n",
        "\n",
        "        Generate insights covering:\n",
        "\n",
        "        1. **Emerging Trends** - What new directions are researchers exploring?\n",
        "        2. **Research Gaps** - What important questions remain unanswered?\n",
        "        3. **Methodological Evolution** - How are research methods changing?\n",
        "        4. **Cross-disciplinary Connections** - How does this field connect to others?\n",
        "        5. **Future Predictions** - Where might this field be heading?\n",
        "\n",
        "        Keep each section to 2-3 paragraphs. Be analytical and forward-thinking.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error generating insights: {e}\")\n",
        "            return \"Research insights generation failed.\"\n",
        "\n",
        "# ========================================\n",
        "# FILE ORGANIZATION CLASS\n",
        "# ========================================\n",
        "\n",
        "class ColabResearchOrganizer:\n",
        "    \"\"\"Handles file organization optimized for Colab\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/research_projects\"):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.base_path.mkdir(exist_ok=True)\n",
        "\n",
        "    def create_project_folder(self, project_name: str) -> Path:\n",
        "        \"\"\"Create a new research project folder\"\"\"\n",
        "        project_path = self.base_path / self.sanitize_filename(project_name)\n",
        "        project_path.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create subfolders\n",
        "        (project_path / \"papers\").mkdir(exist_ok=True)\n",
        "        (project_path / \"summaries\").mkdir(exist_ok=True)\n",
        "        (project_path / \"notes\").mkdir(exist_ok=True)\n",
        "        (project_path / \"data\").mkdir(exist_ok=True)\n",
        "        (project_path / \"insights\").mkdir(exist_ok=True)\n",
        "\n",
        "        return project_path\n",
        "\n",
        "    def sanitize_filename(self, filename: str) -> str:\n",
        "        \"\"\"Sanitize filename for cross-platform compatibility\"\"\"\n",
        "        invalid_chars = '<>:\"/\\\\|?*'\n",
        "        for char in invalid_chars:\n",
        "            filename = filename.replace(char, '_')\n",
        "        return filename[:50]\n",
        "\n",
        "    def save_paper_data(self, project_path: Path, papers: List[Paper]):\n",
        "        \"\"\"Save raw paper data\"\"\"\n",
        "        papers_data = []\n",
        "        for paper in papers:\n",
        "            papers_data.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors,\n",
        "                \"abstract\": paper.abstract,\n",
        "                \"url\": paper.url,\n",
        "                \"published_date\": paper.published_date,\n",
        "                \"source\": paper.source,\n",
        "                \"doi\": paper.doi,\n",
        "                \"categories\": paper.categories\n",
        "            })\n",
        "\n",
        "        with open(project_path / \"data\" / \"papers_data.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(papers_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(papers)} papers to papers_data.json\")\n",
        "\n",
        "    def save_summaries(self, project_path: Path, papers: List[Paper], summaries: List[Dict]):\n",
        "        \"\"\"Save paper summaries\"\"\"\n",
        "        all_summaries = []\n",
        "\n",
        "        for paper, summary in zip(papers, summaries):\n",
        "            filename = self.sanitize_filename(paper.title) + \".json\"\n",
        "\n",
        "            summary_data = {\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": paper.authors,\n",
        "                \"url\": paper.url,\n",
        "                \"source\": paper.source,\n",
        "                \"published_date\": paper.published_date,\n",
        "                \"summary\": summary,\n",
        "                \"generated_on\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Save individual summary\n",
        "            with open(project_path / \"summaries\" / filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            all_summaries.append(summary_data)\n",
        "\n",
        "        # Save consolidated summaries\n",
        "        with open(project_path / \"data\" / \"all_summaries.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_summaries, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Saved {len(summaries)} summaries\")\n",
        "\n",
        "    def save_literature_review(self, project_path: Path, review: str, query: str):\n",
        "        \"\"\"Save literature review\"\"\"\n",
        "        review_content = f\"\"\"# Literature Review: {query}\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Research Query:** {query}\n",
        "**AI Model:** Google Gemini\n",
        "\n",
        "---\n",
        "\n",
        "{review}\n",
        "\n",
        "---\n",
        "\n",
        "*This literature review was automatically generated by the Personal Research Assistant using Google's Gemini AI.*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"notes\" / \"literature_review.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(review_content)\n",
        "\n",
        "        print(\"Literature review saved\")\n",
        "\n",
        "    def save_research_insights(self, project_path: Path, insights: str, query: str):\n",
        "        \"\"\"Save research insights\"\"\"\n",
        "        insights_content = f\"\"\"# Research Insights: {query}\n",
        "\n",
        "**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Research Query:** {query}\n",
        "**AI Model:** Google Gemini\n",
        "\n",
        "---\n",
        "\n",
        "{insights}\n",
        "\n",
        "---\n",
        "\n",
        "*These insights were automatically generated by the Personal Research Assistant using Google's Gemini AI.*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"insights\" / \"research_insights.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(insights_content)\n",
        "\n",
        "        print(\"Research insights saved\")\n",
        "\n",
        "    def create_project_summary(self, project_path: Path, query: str, papers: List[Paper]):\n",
        "        \"\"\"Create a comprehensive project summary\"\"\"\n",
        "        sources = {}\n",
        "        for paper in papers:\n",
        "            sources[paper.source] = sources.get(paper.source, 0) + 1\n",
        "\n",
        "        summary_content = f\"\"\"# Research Project: {query}\n",
        "\n",
        "**Created:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Query:** \"{query}\"\n",
        "**Papers Found:** {len(papers)}\n",
        "**AI Model:** Google Gemini (Free API)\n",
        "\n",
        "##  Paper Sources\n",
        "\"\"\"\n",
        "\n",
        "        for source, count in sources.items():\n",
        "            summary_content += f\"- **{source}:** {count} papers\\n\"\n",
        "\n",
        "        summary_content += f\"\"\"\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "{project_path.name}/\n",
        "├──  README.md (this file)\n",
        "├── data/\n",
        "│   ├── papers_data.json      # Raw paper metadata\n",
        "│   └── all_summaries.json    # Consolidated summaries\n",
        "├── summaries/                # Individual paper analyses\n",
        "├── notes/\n",
        "│   └── literature_review.md  # Generated literature review\n",
        "├── insights/\n",
        "│   └── research_insights.md  # Research trends and patterns\n",
        "└── papers/                   # Additional paper files\n",
        "```\n",
        "\n",
        "## Papers Analyzed\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        for i, paper in enumerate(papers, 1):\n",
        "            authors_str = ', '.join(paper.authors[:3])\n",
        "            if len(paper.authors) > 3:\n",
        "                authors_str += f\" et al. ({len(paper.authors)} total)\"\n",
        "\n",
        "            summary_content += f\"\"\"### {i}. {paper.title}\n",
        "\n",
        "- **Authors:** {authors_str}\n",
        "- **Source:** {paper.source}\n",
        "- **Published:** {paper.published_date}\n",
        "- **URL:** [{paper.url}]({paper.url})\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        summary_content += f\"\"\"\n",
        "## Next Steps\n",
        "\n",
        "1. **Review Literature Review** - Check `notes/literature_review.md` for comprehensive analysis\n",
        "2. **Explore Research Insights** - Browse `insights/research_insights.md` for trends and patterns\n",
        "3. **Examine Individual Summaries** - Browse the `summaries/` folder for detailed paper analyses\n",
        "4. **Download Results** - Use the download function to save your research locally\n",
        "5. **Extend Research** - Add more papers or create additional analyses\n",
        "\n",
        "##  How to Download\n",
        "\n",
        "Run the following code to download your complete research project:\n",
        "\n",
        "```python\n",
        "research_assistant.download_project(\"{project_path.name}\")\n",
        "```\n",
        "\n",
        "## Powered By\n",
        "\n",
        "- **AI Model:** Google Gemini (Free API)\n",
        "- **Paper Sources:** ArXiv, Semantic Scholar, PubMed\n",
        "- **Environment:** Google Colab\n",
        "\n",
        "---\n",
        "*Generated by Personal Research Assistant for Google Colab*\n",
        "\"\"\"\n",
        "\n",
        "        with open(project_path / \"README.md\", 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_content)\n",
        "\n",
        "        print(\"Project summary created\")\n",
        "\n",
        "    def create_downloadable_zip(self, project_path: Path) -> str:\n",
        "        \"\"\"Create a downloadable zip file of the project\"\"\"\n",
        "        zip_path = f\"/content/{project_path.name}.zip\"\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file_path in project_path.rglob('*'):\n",
        "                if file_path.is_file():\n",
        "                    arcname = file_path.relative_to(project_path.parent)\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "        return zip_path\n",
        "\n",
        "# ========================================\n",
        "# MAIN RESEARCH ASSISTANT CLASS\n",
        "# ========================================\n",
        "\n",
        "class ColabResearchAssistant:\n",
        "    \"\"\"Main research assistant optimized for Google Colab with Gemini API\"\"\"\n",
        "\n",
        "    def __init__(self, gemini_api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
        "        self.fetcher = PaperFetcher()\n",
        "        self.summarizer = GeminiResearchSummarizer(gemini_api_key, model_name)\n",
        "        self.organizer = ColabResearchOrganizer()\n",
        "        self.current_project_path = None\n",
        "\n",
        "    def conduct_research(self, query: str, max_papers_per_source: int = 8,\n",
        "                        project_name: Optional[str] = None,\n",
        "                        sources: List[str] = [\"arxiv\", \"semantic_scholar\", \"pubmed\"]) -> Path:\n",
        "        \"\"\"Conduct complete research process with progress indicators\"\"\"\n",
        "\n",
        "        if not project_name:\n",
        "            project_name = f\"{query.replace(' ', '_')[:30]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "        print(\"STARTING RESEARCH PROJECT\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Project: {project_name}\")\n",
        "        print(f\" Query: '{query}'\")\n",
        "        print(f\"Max papers per source: {max_papers_per_source}\")\n",
        "        print(f\"Sources: {', '.join(sources)}\")\n",
        "        print(f\"AI Model: Google Gemini\")\n",
        "        print()\n",
        "\n",
        "        # Create project folder\n",
        "        project_path = self.organizer.create_project_folder(project_name)\n",
        "        self.current_project_path = project_path\n",
        "        print(f\"Project folder created: {project_path}\")\n",
        "        print()\n",
        "\n",
        "        # Fetch papers from selected sources\n",
        "        all_papers = []\n",
        "\n",
        "        if \"arxiv\" in sources:\n",
        "            arxiv_papers = self.fetcher.fetch_arxiv_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(arxiv_papers)\n",
        "\n",
        "        if \"semantic_scholar\" in sources:\n",
        "            ss_papers = self.fetcher.fetch_semantic_scholar_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(ss_papers)\n",
        "\n",
        "        if \"pubmed\" in sources:\n",
        "            pubmed_papers = self.fetcher.fetch_pubmed_papers(query, max_papers_per_source)\n",
        "            all_papers.extend(pubmed_papers)\n",
        "\n",
        "        if not all_papers:\n",
        "            print(\"No papers found for the given query\")\n",
        "            return project_path\n",
        "\n",
        "        print(f\"\\n PAPER COLLECTION SUMMARY\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Total papers found: {len(all_papers)}\")\n",
        "\n",
        "        # Remove duplicates\n",
        "        unique_papers = self.remove_duplicate_papers(all_papers)\n",
        "        print(f\"Unique papers after deduplication: {len(unique_papers)}\")\n",
        "        print()\n",
        "\n",
        "        # Save paper data\n",
        "        self.organizer.save_paper_data(project_path, unique_papers)\n",
        "\n",
        "        # Generate summaries with progress tracking\n",
        "        print(\"GENERATING AI SUMMARIES WITH GEMINI\")\n",
        "        print(\"-\" * 40)\n",
        "        summaries = []\n",
        "        for i, paper in enumerate(unique_papers, 1):\n",
        "            print(f\" Processing {i}/{len(unique_papers)}: {paper.title[:50]}...\")\n",
        "            summary = self.summarizer.summarize_paper(paper)\n",
        "            summaries.append(summary)\n",
        "\n",
        "            # Rate limiting for Gemini API (more generous than OpenAI)\n",
        "            if i < len(unique_papers):\n",
        "                time.sleep(1)  # Reduced delay\n",
        "\n",
        "        print(f\"Generated {len(summaries)} paper summaries\")\n",
        "        print()\n",
        "\n",
        "        # Save summaries\n",
        "        self.organizer.save_summaries(project_path, unique_papers, summaries)\n",
        "\n",
        "        # Generate literature review\n",
        "        print(\"GENERATING LITERATURE REVIEW\")\n",
        "        print(\"-\" * 30)\n",
        "        literature_review = self.summarizer.generate_literature_review(unique_papers, summaries, query)\n",
        "        self.organizer.save_literature_review(project_path, literature_review, query)\n",
        "        print(\" Literature review generated\")\n",
        "        print()\n",
        "\n",
        "        # Generate research insights\n",
        "        print(\"GENERATING RESEARCH INSIGHTS\")\n",
        "        print(\"-\" * 30)\n",
        "        insights = self.summarizer.generate_research_insights(unique_papers, summaries, query)\n",
        "        self.organizer.save_research_insights(project_path, insights, query)\n",
        "        print(\"Research insights generated\")\n",
        "        print()\n",
        "\n",
        "        # Create project summary\n",
        "        self.organizer.create_project_summary(project_path, query, unique_papers)\n",
        "\n",
        "        print(\"RESEARCH PROJECT COMPLETED!\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Location: {project_path}\")\n",
        "        print(f\"Papers analyzed: {len(unique_papers)}\")\n",
        "        print(f\"Files generated: {len(list(project_path.rglob('*')))} files\")\n",
        "        print(f\"Powered by: Google Gemini API\")\n",
        "        print()\n",
        "        print(\"To explore your results:\")\n",
        "        print(f\"   • Project overview: {project_path}/README.md\")\n",
        "        print(f\"   • Literature review: {project_path}/notes/literature_review.md\")\n",
        "        print(f\"   • Research insights: {project_path}/insights/research_insights.md\")\n",
        "        print(f\"   • Paper summaries: {project_path}/summaries/\")\n",
        "        print()\n",
        "\n",
        "        return project_path\n",
        "\n",
        "    def remove_duplicate_papers(self, papers: List[Paper]) -> List[Paper]:\n",
        "        \"\"\"Remove duplicate papers based on title similarity\"\"\"\n",
        "        unique_papers = []\n",
        "        seen_titles = set()\n",
        "\n",
        "        for paper in papers:\n",
        "            normalized_title = paper.title.lower().strip().replace(' ', '')\n",
        "            if normalized_title not in seen_titles:\n",
        "                seen_titles.add(normalized_title)\n",
        "                unique_papers.append(paper)\n",
        "\n",
        "        return unique_papers\n",
        "\n",
        "    def display_project_summary(self, project_path: Path = None):\n",
        "        \"\"\"Display a nice summary of the research project\"\"\"\n",
        "        if project_path is None:\n",
        "            project_path = self.current_project_path\n",
        "\n",
        "        if not project_path or not project_path.exists():\n",
        "            print(\"No project found to display\")\n",
        "            return\n",
        "\n",
        "        # Read project data\n",
        "        papers_file = project_path / \"data\" / \"papers_data.json\"\n",
        "        if papers_file.exists():\n",
        "            with open(papers_file, 'r') as f:\n",
        "                papers_data = json.load(f)\n",
        "\n",
        "            print(\"PROJECT SUMMARY\")\n",
        "            print(\"=\" * 40)\n",
        "            print(f\" Project: {project_path.name}\")\n",
        "            print(f\" Papers: {len(papers_data)}\")\n",
        "\n",
        "            # Source breakdown\n",
        "            sources = {}\n",
        "            for paper in papers_data:\n",
        "                source = paper['source']\n",
        "                sources[source] = sources.get(source, 0) + 1\n",
        "\n",
        "            print(\"\\n Sources:\")\n",
        "            for source, count in sources.items():\n",
        "                print(f\"   • {source}: {count} papers\")\n",
        "\n",
        "            print(f\"\\n Files created: {len(list(project_path.rglob('*')))} files\")\n",
        "            print(f\" Total size: {self.get_folder_size(project_path):.2f} MB\")\n",
        "            print(f\" AI Model: Google Gemini\")\n",
        "\n",
        "    def get_folder_size(self, folder_path: Path) -> float:\n",
        "        \"\"\"Calculate folder size in MB\"\"\"\n",
        "        total_size = 0\n",
        "        for file_path in folder_path.rglob('*'):\n",
        "            if file_path.is_file():\n",
        "                total_size += file_path.stat().st_size\n",
        "        return total_size / (1024 * 1024)\n",
        "\n",
        "    def download_project(self, project_name: str = None):\n",
        "        \"\"\"Create and download a zip file of the research project\"\"\"\n",
        "        if project_name:\n",
        "            project_path = self.organizer.base_path / project_name\n",
        "        else:\n",
        "            project_path = self.current_project_path\n",
        "\n",
        "        if not project_path or not project_path.exists():\n",
        "            print(\"Project not found\")\n",
        "            return\n",
        "\n",
        "        print(f\"Creating download package for: {project_path.name}\")\n",
        "        zip_path = self.organizer.create_downloadable_zip(project_path)\n",
        "\n",
        "        print(f\"Download package ready!\")\n",
        "        print(f\" Size: {Path(zip_path).stat().st_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "        # Download the file\n",
        "        files.download(zip_path)\n",
        "        print(\" Download started! Check your browser's downloads folder.\")\n",
        "\n",
        "    def quick_search(self, query: str, source: str = \"arxiv\", max_results: int = 5) -> List[Dict]:\n",
        "        \"\"\"Quick search function for immediate results without full processing\"\"\"\n",
        "        print(f\"Quick search: '{query}' from {source}\")\n",
        "\n",
        "        papers = []\n",
        "        if source.lower() == \"arxiv\":\n",
        "            papers = self.fetcher.fetch_arxiv_papers(query, max_results)\n",
        "        elif source.lower() == \"semantic_scholar\":\n",
        "            papers = self.fetcher.fetch_semantic_scholar_papers(query, max_results)\n",
        "        elif source.lower() == \"pubmed\":\n",
        "            papers = self.fetcher.fetch_pubmed_papers(query, max_results)\n",
        "\n",
        "        # Return simplified data\n",
        "        results = []\n",
        "        for paper in papers:\n",
        "            results.append({\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": \", \".join(paper.authors[:3]),\n",
        "                \"source\": paper.source,\n",
        "                \"url\": paper.url,\n",
        "                \"abstract\": paper.abstract[:200] + \"...\" if len(paper.abstract) > 200 else paper.abstract\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def list_projects(self):\n",
        "        \"\"\"List all existing research projects\"\"\"\n",
        "        projects = [d for d in self.organizer.base_path.iterdir() if d.is_dir()]\n",
        "\n",
        "        if not projects:\n",
        "            print(\"No research projects found.\")\n",
        "            return\n",
        "\n",
        "        print(\"EXISTING RESEARCH PROJECTS\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        for i, project in enumerate(sorted(projects), 1):\n",
        "            # Try to read project info\n",
        "            readme_path = project / \"README.md\"\n",
        "            papers_path = project / \"data\" / \"papers_data.json\"\n",
        "\n",
        "            paper_count = 0\n",
        "            if papers_path.exists():\n",
        "                try:\n",
        "                    with open(papers_path, 'r') as f:\n",
        "                        paper_count = len(json.load(f))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            size_mb = self.get_folder_size(project)\n",
        "            print(f\"{i}. {project.name}\")\n",
        "            print(f\"   Papers: {paper_count} | 💾 Size: {size_mb:.1f}MB\")\n",
        "            print(f\"   Path: {project}\")\n",
        "            print()\n",
        "\n",
        "# ========================================\n",
        "# EASY-TO-USE FUNCTIONS FOR COLAB\n",
        "# ========================================\n",
        "\n",
        "def quick_research(query: str, max_papers: int = 10, model: str = \"gemini-1.5-flash\"):\n",
        "    \"\"\"\n",
        "    Quick research function - just provide a query!\n",
        "\n",
        "    Args:\n",
        "        query (str): Your research question or topic\n",
        "        max_papers (int): Maximum papers per source (default: 10)\n",
        "        model (str): Gemini model to use (default: gemini-1.5-flash for speed)\n",
        "\n",
        "    Returns:\n",
        "        ColabResearchAssistant: The research assistant instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup configuration\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    # Initialize research assistant\n",
        "    assistant = ColabResearchAssistant(api_key, model)\n",
        "\n",
        "    # Conduct research\n",
        "    project_path = assistant.conduct_research(\n",
        "        query=query,\n",
        "        max_papers_per_source=max_papers,\n",
        "        sources=[\"arxiv\", \"semantic_scholar\", \"pubmed\"]\n",
        "    )\n",
        "\n",
        "    # Display summary\n",
        "    assistant.display_project_summary()\n",
        "\n",
        "    return assistant\n",
        "\n",
        "def setup_research_environment():\n",
        "    \"\"\"Setup the research environment and return a configured assistant\"\"\"\n",
        "    print(\"🔧 SETTING UP RESEARCH ENVIRONMENT\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    assistant = ColabResearchAssistant(api_key)\n",
        "\n",
        "    print(\"Environment ready!\")\n",
        "    print(\"\\n🚀 Quick start:\")\n",
        "    print(\"assistant.conduct_research('your research topic')\")\n",
        "    print(\"\\n📚 Or use the quick function:\")\n",
        "    print(\"quick_research('your research topic')\")\n",
        "\n",
        "    return assistant\n",
        "\n",
        "def quick_search(query: str, source: str = \"arxiv\", max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Quick search without full processing - just get paper titles and abstracts\n",
        "\n",
        "    Args:\n",
        "        query (str): Search query\n",
        "        source (str): \"arxiv\", \"semantic_scholar\", or \"pubmed\"\n",
        "        max_results (int): Number of results\n",
        "    \"\"\"\n",
        "    config = ColabConfig()\n",
        "    api_key = config.setup_gemini_key()\n",
        "\n",
        "    assistant = ColabResearchAssistant(api_key)\n",
        "    results = assistant.quick_search(query, source, max_results)\n",
        "\n",
        "    print(f\"🔍 Quick Search Results: '{query}' from {source}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, paper in enumerate(results, 1):\n",
        "        print(f\"{i}. **{paper['title']}**\")\n",
        "        print(f\"   Authors: {paper['authors']}\")\n",
        "        print(f\"   Source: {paper['source']}\")\n",
        "        print(f\"   Abstract: {paper['abstract']}\")\n",
        "        print(f\"   URL: {paper['url']}\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "# ========================================\n",
        "# EXAMPLE USAGE AND INSTRUCTIONS\n",
        "# ========================================\n",
        "\n",
        "def show_examples():\n",
        "    \"\"\"Show example usage patterns\"\"\"\n",
        "    print(\"\"\"\n",
        "🔬 PERSONAL RESEARCH ASSISTANT - GEMINI VERSION\n",
        "===============================================\n",
        "\n",
        "🆓 **FREE Google Gemini API** - Get your key at: https://aistudio.google.com/app/apikey\n",
        "\n",
        "## 🚀 QUICK START EXAMPLES\n",
        "\n",
        "1. **INSTANT RESEARCH** (Easiest method):\n",
        "   ```python\n",
        "   assistant = quick_research(\"machine learning in healthcare\", max_papers=15)\n",
        "   ```\n",
        "\n",
        "2. **SETUP ONCE, USE MULTIPLE TIMES**:\n",
        "   ```python\n",
        "   assistant = setup_research_environment()\n",
        "   assistant.conduct_research(\"quantum computing algorithms\", max_papers_per_source=12)\n",
        "   ```\n",
        "\n",
        "3. **QUICK PAPER SEARCH** (No AI processing):\n",
        "   ```python\n",
        "   results = quick_search(\"neural networks\", source=\"arxiv\", max_results=10)\n",
        "   ```\n",
        "\n",
        "4. **CUSTOM RESEARCH WITH MULTIPLE SOURCES**:\n",
        "   ```python\n",
        "   assistant.conduct_research(\n",
        "       query=\"natural language processing transformers\",\n",
        "       max_papers_per_source=20,\n",
        "       project_name=\"NLP_Transformers_2024\",\n",
        "       sources=[\"arxiv\", \"semantic_scholar\", \"pubmed\"]\n",
        "   )\n",
        "   ```\n",
        "\n",
        "## 📥 DOWNLOAD & MANAGE\n",
        "\n",
        "```python\n",
        "# Download current project\n",
        "assistant.download_project()\n",
        "\n",
        "# Download specific project\n",
        "assistant.download_project(\"specific_project_name\")\n",
        "\n",
        "# List all projects\n",
        "assistant.list_projects()\n",
        "```\n",
        "\n",
        "## 🆕 NEW FEATURES\n",
        "\n",
        "✅ **Google Gemini AI** - Free API with generous limits\n",
        "✅ **PubMed Integration** - Medical/biological research papers\n",
        "✅ **Research Insights** - AI-generated trends and patterns analysis\n",
        "✅ **Quick Search** - Fast paper discovery without full processing\n",
        "✅ **Project Management** - List, download, and organize multiple projects\n",
        "\n",
        "## 📊 SOURCES AVAILABLE\n",
        "\n",
        "- **ArXiv** - Physics, Math, Computer Science, Biology\n",
        "- **Semantic Scholar** - Multidisciplinary academic papers\n",
        "- **PubMed** - Medical and life sciences literature\n",
        "\n",
        "## 💡 PRO TIPS\n",
        "\n",
        "• **Start small**: Begin with 8-12 papers per source for faster results\n",
        "• **Use specific keywords**: \"machine learning healthcare\" vs \"AI medicine\"\n",
        "• **Try different sources**: Each has different paper collections\n",
        "• **Download results**: Save your work locally for future reference\n",
        "• **Check insights**: New AI-generated research trends analysis\n",
        "\n",
        "## 🔧 MODELS AVAILABLE\n",
        "\n",
        "- **gemini-1.5-flash** (default) - Fast and efficient\n",
        "- **gemini-1.5-pro** - More detailed analysis (slower)\n",
        "\n",
        "## 📈 COST COMPARISON\n",
        "\n",
        "**Google Gemini FREE**: 15 requests/minute, 1500 requests/day\n",
        "**OpenAI GPT**: $0.002-0.06 per 1K tokens (paid only)\n",
        "\n",
        "## 🚨 GETTING STARTED\n",
        "\n",
        "```python\n",
        "# Just run this and follow the prompts!\n",
        "quick_research(\"your research topic here\")\n",
        "```\n",
        "\n",
        "Happy researching! 🎓\n",
        "    \"\"\")\n",
        "\n",
        "def show_gemini_setup():\n",
        "    \"\"\"Show detailed Gemini API setup instructions\"\"\"\n",
        "    print(\"\"\"\n",
        "🔑 GOOGLE GEMINI API SETUP GUIDE\n",
        "================================\n",
        "\n",
        "## Step 1: Get Your Free API Key\n",
        "1. Go to: https://aistudio.google.com/app/apikey\n",
        "2. Sign in with your Google account\n",
        "3. Click \"Create API Key\"\n",
        "4. Copy your API key (starts with 'AI...')\n",
        "\n",
        "## Step 2: Run the Setup\n",
        "```python\n",
        "assistant = setup_research_environment()\n",
        "```\n",
        "\n",
        "## Step 3: Enter Your Key\n",
        "- Paste your API key when prompted\n",
        "- It will be securely stored for this session\n",
        "\n",
        "## ✅ You're Ready!\n",
        "```python\n",
        "quick_research(\"your topic\")\n",
        "```\n",
        "\n",
        "## 🆓 Free Limits (Very Generous!)\n",
        "- 15 requests per minute\n",
        "- 1,500 requests per day\n",
        "- No credit card required\n",
        "\n",
        "## 🔒 Privacy\n",
        "- API keys are stored only in your Colab session\n",
        "- Not saved permanently\n",
        "- Google's standard privacy policies apply\n",
        "    \"\"\")\n",
        "\n",
        "# Auto-display examples when notebook loads\n",
        "print(\"🎯 Personal Research Assistant - Gemini Version Ready!\")\n",
        "print(\"🆓 Powered by Google's FREE Gemini API\")\n",
        "print()\n",
        "print(\"📚 Type: show_examples() to see usage examples\")\n",
        "print(\"🔑 Type: show_gemini_setup() for API setup help\")\n",
        "print(\"🚀 Type: quick_research('your topic') to start immediately\")\n",
        "print()\n",
        "print(\"Get your FREE Gemini API key: https://aistudio.google.com/app/apikey\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc6F2KUDM8Lj",
        "outputId": "9fd49263-91fa-48ed-9769-2ea5cbc7a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Personal Research Assistant - Gemini Version Ready!\n",
            "🆓 Powered by Google's FREE Gemini API\n",
            "\n",
            "📚 Type: show_examples() to see usage examples\n",
            "🔑 Type: show_gemini_setup() for API setup help\n",
            "🚀 Type: quick_research('your topic') to start immediately\n",
            "\n",
            "Get your FREE Gemini API key: https://aistudio.google.com/app/apikey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the research assistant code above first, then use:\n",
        "\n",
        "# Method 1: Super quick research\n",
        "assistant = quick_research(\"Microprocessor\", max_papers=10)\n",
        "\n",
        "# Method 2: Step-by-step setup\n",
        "assistant = setup_research_environment()\n",
        "assistant.conduct_research(\"RNN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K0iiQPZsRpnx",
        "outputId": "505472c2-6fe4-4dcb-bf5d-5ee565d55f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Gemini API Key Setup\n",
            "========================================\n",
            "Get your FREE API key at: https://aistudio.google.com/app/apikey\n",
            "\n",
            "Gemini API key already configured!\n",
            "STARTING RESEARCH PROJECT\n",
            "==================================================\n",
            "Project: Microprocessor_20250824_095220\n",
            " Query: 'Microprocessor'\n",
            "Max papers per source: 10\n",
            "Sources: arxiv, semantic_scholar, pubmed\n",
            "AI Model: Google Gemini\n",
            "\n",
            "Project folder created: /content/research_projects/Microprocessor_20250824_095220\n",
            "\n",
            "Searching ArXiv for: 'Microprocessor'\n",
            "Found 10 papers from ArXiv\n",
            "Searching Semantic Scholar for: 'Microprocessor'\n",
            "Found 2 papers from Semantic Scholar\n",
            "Searching PubMed for: 'Microprocessor'\n",
            "Found 10 papers from PubMed\n",
            "\n",
            " PAPER COLLECTION SUMMARY\n",
            "------------------------------\n",
            "Total papers found: 22\n",
            "Unique papers after deduplication: 22\n",
            "\n",
            "Saved 22 papers to papers_data.json\n",
            "GENERATING AI SUMMARIES WITH GEMINI\n",
            "----------------------------------------\n",
            " Processing 1/22: Altermagnetic spintronics...\n",
            "Summarizing: Altermagnetic spintronics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 708.96ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 2/22: Design and Simulation of 6T SRAM Array...\n",
            "Summarizing: Design and Simulation of 6T SRAM Array...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 910.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 3/22: Global Microprocessor Correctness in the Presence ...\n",
            "Summarizing: Global Microprocessor Correctness in the Presence of Transie...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 4/22: Detecting Hardware Trojans in Microprocessors via ...\n",
            "Summarizing: Detecting Hardware Trojans in Microprocessors via Hardware E...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 658.10ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 5/22: Role of Uncertainty in Model Development and Contr...\n",
            "Summarizing: Role of Uncertainty in Model Development and Control Design ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 759.17ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 6/22: Control Architecture and Design for a Multi-roboti...\n",
            "Summarizing: Control Architecture and Design for a Multi-robotic Visual S...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 7/22: Barium Calcium Zirconium Titanate Thin Film-Based ...\n",
            "Summarizing: Barium Calcium Zirconium Titanate Thin Film-Based Capacitive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 684.40ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 8/22: PGR-DRC: Pre-Global Routing DRC Violation Predicti...\n",
            "Summarizing: PGR-DRC: Pre-Global Routing DRC Violation Prediction Using U...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 9/22: High Throughput Event Filtering: The Interpolation...\n",
            "Summarizing: High Throughput Event Filtering: The Interpolation-based DIF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 10/22: Towards Mixed-Criticality Software Architectures f...\n",
            "Summarizing: Towards Mixed-Criticality Software Architectures for Central...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 11/22: Powering a microprocessor by photosynthesis...\n",
            "Summarizing: Powering a microprocessor by photosynthesis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 658.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 12/22: A Programmable Heterogeneous Microprocessor Based ...\n",
            "Summarizing: A Programmable Heterogeneous Microprocessor Based on Bit-Sca...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 13/22: CRISPR-based fluorescent aptasensor combined with ...\n",
            "Summarizing: CRISPR-based fluorescent aptasensor combined with smartphone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 834.79ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 14/22: Technological features of smartphone apps for phys...\n",
            "Summarizing: Technological features of smartphone apps for physical activ...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 15/22: A large Stokes shift peptide-based fluorescent pro...\n",
            "Summarizing: A large Stokes shift peptide-based fluorescent probe for Cd...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 861.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 16/22: Rapid visual authentication of high-temperature Da...\n",
            "Summarizing: Rapid visual authentication of high-temperature Daqu Baijiu ...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 17/22: N, P and S co-doped red fluorescent carbon dots se...\n",
            "Summarizing: N, P and S co-doped red fluorescent carbon dots sensing for ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 682.82ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 18/22: High sensitive and selective multivariate detectio...\n",
            "Summarizing: High sensitive and selective multivariate detection of Hg...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 19/22: Association between smartphone screen time and exa...\n",
            "Summarizing: Association between smartphone screen time and exaggerated b...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 758.84ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 20/22: Triple-synergistic hollow AuAg@CeO...\n",
            "Summarizing: Triple-synergistic hollow AuAg@CeO...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 21/22: Simple one-pot assay for visual and smartphone-bas...\n",
            "Summarizing: Simple one-pot assay for visual and smartphone-based quantif...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1012.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 22/22: A simple and low-cost paper chip-based smartphone ...\n",
            "Summarizing: A simple and low-cost paper chip-based smartphone sensor for...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Generated 22 paper summaries\n",
            "\n",
            "Saved 22 summaries\n",
            "GENERATING LITERATURE REVIEW\n",
            "------------------------------\n",
            "Generating literature review...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 835.21ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating literature review: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            "Literature review saved\n",
            " Literature review generated\n",
            "\n",
            "GENERATING RESEARCH INSIGHTS\n",
            "------------------------------\n",
            "Generating research insights...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.45ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Error generating insights: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            "Research insights saved\n",
            "Research insights generated\n",
            "\n",
            "Project summary created\n",
            "RESEARCH PROJECT COMPLETED!\n",
            "==================================================\n",
            "Location: /content/research_projects/Microprocessor_20250824_095220\n",
            "Papers analyzed: 22\n",
            "Files generated: 32 files\n",
            "Powered by: Google Gemini API\n",
            "\n",
            "To explore your results:\n",
            "   • Project overview: /content/research_projects/Microprocessor_20250824_095220/README.md\n",
            "   • Literature review: /content/research_projects/Microprocessor_20250824_095220/notes/literature_review.md\n",
            "   • Research insights: /content/research_projects/Microprocessor_20250824_095220/insights/research_insights.md\n",
            "   • Paper summaries: /content/research_projects/Microprocessor_20250824_095220/summaries/\n",
            "\n",
            "PROJECT SUMMARY\n",
            "========================================\n",
            " Project: Microprocessor_20250824_095220\n",
            " Papers: 22\n",
            "\n",
            " Sources:\n",
            "   • ArXiv: 10 papers\n",
            "   • Semantic Scholar: 2 papers\n",
            "   • PubMed: 10 papers\n",
            "\n",
            " Files created: 32 files\n",
            " Total size: 0.09 MB\n",
            " AI Model: Google Gemini\n",
            "🔧 SETTING UP RESEARCH ENVIRONMENT\n",
            "========================================\n",
            "Google Gemini API Key Setup\n",
            "========================================\n",
            "Get your FREE API key at: https://aistudio.google.com/app/apikey\n",
            "\n",
            "Gemini API key already configured!\n",
            "Environment ready!\n",
            "\n",
            "🚀 Quick start:\n",
            "assistant.conduct_research('your research topic')\n",
            "\n",
            "📚 Or use the quick function:\n",
            "quick_research('your research topic')\n",
            "STARTING RESEARCH PROJECT\n",
            "==================================================\n",
            "Project: RNN_20250824_095253\n",
            " Query: 'RNN'\n",
            "Max papers per source: 8\n",
            "Sources: arxiv, semantic_scholar, pubmed\n",
            "AI Model: Google Gemini\n",
            "\n",
            "Project folder created: /content/research_projects/RNN_20250824_095253\n",
            "\n",
            "Searching ArXiv for: 'RNN'\n",
            "Found 8 papers from ArXiv\n",
            "Searching Semantic Scholar for: 'RNN'\n",
            "Found 7 papers from Semantic Scholar\n",
            "Searching PubMed for: 'RNN'\n",
            "Found 8 papers from PubMed\n",
            "\n",
            " PAPER COLLECTION SUMMARY\n",
            "------------------------------\n",
            "Total papers found: 23\n",
            "Unique papers after deduplication: 23\n",
            "\n",
            "Saved 23 papers to papers_data.json\n",
            "GENERATING AI SUMMARIES WITH GEMINI\n",
            "----------------------------------------\n",
            " Processing 1/23: Investment Portfolio Optimization Based on Modern ...\n",
            "Summarizing: Investment Portfolio Optimization Based on Modern Portfolio ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.46ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 2/23: The Digital Sous Chef -- A Comparative Study on Fi...\n",
            "Summarizing: The Digital Sous Chef -- A Comparative Study on Fine-Tuning ...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 3/23: Time-Scale Coupling Between States and Parameters ...\n",
            "Summarizing: Time-Scale Coupling Between States and Parameters in Recurre...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.43ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 4/23: Neurosymbolic Learning for Predicting Cell Fate De...\n",
            "Summarizing: Neurosymbolic Learning for Predicting Cell Fate Decisions fr...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 5/23: HyperTea: A Hypergraph-based Temporal Enhancement ...\n",
            "Summarizing: HyperTea: A Hypergraph-based Temporal Enhancement and Alignm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 661.37ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 6/23: A Novel Study on Intelligent Methods and Explainab...\n",
            "Summarizing: A Novel Study on Intelligent Methods and Explainable AI for ...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 7/23: Residual Reservoir Memory Networks...\n",
            "Summarizing: Residual Reservoir Memory Networks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1845.30ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 8/23: CKFNet: Neural Network Aided Cubature Kalman filte...\n",
            "Summarizing: CKFNet: Neural Network Aided Cubature Kalman filtering...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 9/23: Learning Phrase Representations using RNN Encoder–...\n",
            "Summarizing: Learning Phrase Representations using RNN Encoder–Decoder fo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.62ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 10/23: A Comprehensive Overview and Comparative Analysis ...\n",
            "Summarizing: A Comprehensive Overview and Comparative Analysis on Deep Le...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 11/23: Electrical Load Forecasting Using LSTM, GRU, and R...\n",
            "Summarizing: Electrical Load Forecasting Using LSTM, GRU, and RNN Algorit...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 684.06ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 12/23: DB-RNN: An RNN for Precipitation Nowcasting Deblur...\n",
            "Summarizing: DB-RNN: An RNN for Precipitation Nowcasting Deblurring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 708.70ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 13/23: Dual-Path RNN: Efficient Long Sequence Modeling fo...\n",
            "Summarizing: Dual-Path RNN: Efficient Long Sequence Modeling for Time-Dom...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 14/23: Transformer Transducer: A Streamable Speech Recogn...\n",
            "Summarizing: Transformer Transducer: A Streamable Speech Recognition Mode...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 658.73ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 15/23: CNN-RNN: A Unified Framework for Multi-label Image...\n",
            "Summarizing: CNN-RNN: A Unified Framework for Multi-label Image Classific...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 16/23: Development of student intent-based educational ch...\n",
            "Summarizing: Development of student intent-based educational chatbot syst...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 684.14ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 17/23: Performance analysis of neural network architectur...\n",
            "Summarizing: Performance analysis of neural network architectures for tim...\n",
            "Error summarizing paper: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            " Processing 18/23: Predictive efficacy of machine-learning algorithms...\n",
            "Summarizing: Predictive efficacy of machine-learning algorithms on intrah...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.32ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 19/23: CWBLS network and its application in portable spec...\n",
            "Summarizing: CWBLS network and its application in portable spectral measu...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 20/23: A Joint Multimodal User Authentication-based Priva...\n",
            "Summarizing: A Joint Multimodal User Authentication-based Privacy Preserv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 682.98ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error summarizing paper: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: API key expired. Please renew the API key.\n",
            " Processing 21/23: Emotion recognition in EEG Signals: Deep and machi...\n",
            "Summarizing: Emotion recognition in EEG Signals: Deep and machine learnin...\n",
            "Error summarizing paper: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            " Processing 22/23: Mechanobiology-guided machine learning models for ...\n",
            "Summarizing: Mechanobiology-guided machine learning models for predicting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 683.34ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4-dBzCGUDg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}